---
title: "Homework 02"
author: "Tingrui Huang"
date: "Septemeber 20, 2018"
output:
  pdf_document: default
---

\newcommand{\mat}[1]{\boldsymbol{#1}} 
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\rv}[1]{\underline{#1}}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,dev="CairoPNG",fig.align = "center", 
                      fig.width = 5.656, fig.height = 4, global.par = TRUE)
pacman::p_load("arm","data.table","Cairo","faraway","foreign","ggplot2","knitr")
par (mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01)
```

# Introduction 
In homework 2 you will fit many regression models.  You are welcome to explore beyond what the question is asking you.  

Please come see us we are here to help.

## Data analysis 

### Analysis of earnings and height data

The folder `earnings` has data from the Work, Family, and Well-Being Survey (Ross, 1990).
You can find the codebook at http://www.stat.columbia.edu/~gelman/arm/examples/earnings/wfwcodebook.txt
```{r}
gelman_dir <- "http://www.stat.columbia.edu/~gelman/arm/examples/"
heights    <- read.dta (paste0(gelman_dir,"earnings/heights.dta"))
```

Pull out the data on earnings, sex, height, and weight.

1. In R, check the dataset and clean any unusually coded data.

```{r}
library(car)
library(carData)
library(arm)
library(faraway)
#Look at the dataset.
summary(heights)

# In the dataset we can find that, the survey was conducted in 1990, and many respondents had age younger than 18,
# which is younger than the legal age for working. Therefore, we need to remove these records.
heights$yearbn[heights$yearbn > 73] <- NA

#There are a lot of NA inputs in the dataset, so we are going to remove these NA values.
na <- which(!complete.cases(heights))
heights_clean_1 <- heights[-na,]

# Since we are going to study the relation between earn and other variables, therefore, if a person's earn is 0,
# then we need to remove it from the dataset
no_income <- which(heights_clean_1$earn==0)
heights_clean <- heights_clean_1[-no_income,]

#Discover outliers by using Bonferroni outlier test
regall <- lm(earn~height1+height2+sex+race+hisp+ed+yearbn+height, data = heights_clean)
outlierTest(regall)
influencePlot(regall)

#Convert sex into 0 for Men and 1 for Women
heights_clean$sex <- heights_clean$sex - 1
View(heights_clean)
```

2. Fit a linear regression model predicting earnings from height. What transformation should you perform in order to interpret the intercept from this model as average earnings for people with average height?

```{r}
#Regress "earn" onto "height"
reg_h_1 <- lm(earn~height, data = heights_clean)
ggplot(reg_h_1)+aes(height,earn)+geom_point()+stat_smooth(method='lm',col='red')

#Since thereis no one's height is zero, therefore, I would center the height to its mean
center_height <- heights_clean$height - mean(heights_clean$height)
reg_h_2 <- lm(earn~center_height, data = heights_clean)
ggplot(reg_h_2)+aes(center_height,earn)+geom_point()+stat_smooth(method='lm',col='red')
summary(reg_h_2)

#Interpretation: in the refined model, for a person with average height (center_height=0) has a income of 23128.3
#And each unit increase in heights, will be resulted in 1271.1 more income.
```

3. Fit some regression models with the goal of predicting earnings from some
combination of sex, height, and weight. Be sure to try various transformations and interactions that might make sense. Choose your preferred model and justify.

```{r}
# Test 1. Put everything in, since all of the three variables could influence earning.
reg_t1 <- lm(earn~height+sex+race+ed+yearbn, data = heights_clean)
summary(reg_t1)

# Test 2. Still put everything in but we are going to assume there are interactions between each of them
reg_t2 <- lm(earn~height*sex*race*ed*yearbn, data=heights_clean)
summary(reg_t2)

# Test 3. Based on test 1 and 2, I select "sex", "ed" and "yearbn" plus centered "height"
reg_t3 <- lm(earn~center_height+sex+ed+yearbn, data = heights_clean)
summary(reg_t3)

# Test 4. Consider there are nteraction between
reg_t4 <- lm(earn~center_height*sex+ed+yearbn, data = heights_clean)
summary(reg_t4)
plot(reg_t4)
marginalModelPlots(reg_t4)

# Test 5. Use log transformation for earn
reg_t5 <- lm(log(earn)~center_height*sex+ed+yearbn, data = heights_clean)
summary(reg_t5)

# According the p-value and plots, I prefer the model 5, which include the centered height, sex, ed and yearbn.
# First of all, I think it's very close to the real world cases. People with better education and older tend to make more money. Meanwhile, males often make more than females when other factors are the same, although this is not fair.
```

4. Interpret all model coefficients.

```{r}
summary(reg_t5)
# Intercept: Intercept represents the average income for a male person with average heights, no education and born in
# the year of 1900.

# sex: Females earn less than males by 44%. 

# education: People with higher education will earn more than people that are less educated. The difference in income
# of each level of education is 12%

# yearbn: older people tend to make more money than younger people.

# center height: height has positive correlation with earn, every unit taller in height will result in 2% increase
# in income.
```

5. Construct 95% confidence interval for all model coefficients and discuss what they mean.

```{r}
confint(reg_t5, level = 0.95)
# The confidence intervals for "intercept", "sex", "ed", and "yearbn" are not across 0, therefore, I would say
# these predictors are more statistically significant than others. Although the CI of height across the 0, 
# I would still consider its influence since it could be a important variable.
```


### Analysis of mortality rates and various environmental factors

The folder `pollution` contains mortality rates and various environmental factors from 60 U.S. metropolitan areas from McDonald, G.C. and Schwing, R.C. (1973) 'Instabilities of regression estimates relating air pollution to mortality', Technometrics, vol.15, 463-482. 

Variables, in order:

* PREC   Average annual precipitation in inches
* JANT   Average January temperature in degrees F
* JULT   Same for July
* OVR65  % of 1960 SMSA population aged 65 or older
* POPN   Average household size
* EDUC   Median school years completed by those over 22
* HOUS   % of housing units which are sound & with all facilities
* DENS   Population per sq. mile in urbanized areas, 1960
* NONW   % non-white population in urbanized areas, 1960
* WWDRK  % employed in white collar occupations
* POOR   % of families with income < $3000
* HC     Relative hydrocarbon pollution potential
* NOX    Same for nitric oxides
* SO@    Same for sulphur dioxide
* HUMID  Annual average % relative humidity at 1pm
* MORT   Total age-adjusted mortality rate per 100,000

For this exercise we shall model mortality rate given nitric oxides, sulfur dioxide, and hydrocarbons as inputs. This model is an extreme oversimplification as it combines all sources of mortality and does not adjust for crucial factors such as age and smoking. We use it to illustrate log transformations in regression.

```{r}
gelman_dir   <- "http://www.stat.columbia.edu/~gelman/arm/examples/"
pollution    <- read.dta (paste0(gelman_dir,"pollution/pollution.dta"))
```

1. Create a scatterplot of mortality rate versus level of nitric oxides. Do you think linear regression will fit these data well? Fit the regression and evaluate a residual plot from the regression.

```{r}
plot(x=pollution$nox,y=pollution$mort)
# Based on the plot, I would say the regression could fit these data, but let's try
pol_t1 <- lm(mort~nox, data = pollution)
summary(pol_t1)
plot(pol_t1)
# The residual plot looks horrible
```

2. Find an appropriate transformation that will result in data more appropriate for linear regression. Fit a regression to the transformed data and evaluate the new residual plot.

```{r}
# We could try the log transformation, square root transormations and reciprocal transformation
pol_t2 <- lm(log(mort)~nox, data = pollution)
pol_t3 <- lm(sqrt(mort)~nox, data = pollution)
pol_t4 <- lm(mort~(1/nox), data=pollution)
pol_t5 <- lm(mort~nox+I(nox^2)+I(nox^3), data=pollution)
plot(pol_t2, which = 1);plot(pol_t3, which = 1);plot(pol_t4, which = 1);plot(pol_t5,which = 1)
# In the residual plot of "pol_t5", although the red line is still not close enough to line 0, but the result
# is much better the others.
```

3. Interpret the slope coefficient from the model you chose in 2.

```{r}
summary(pol_t5)
# The slope coefficient tells that nox has a positive and significant relationship with mortality rate.
# For every unit increase in nox, the mortality rate will increase 2.582e+00
```

4. Construct 99% confidence interval for slope coefficient from the model you chose in 2 and interpret them.

```{r}
confint(pol_t5, level = 0.99)
# As we can see in ths table, the CI of nox doesn't across 0, therefore, it's statistically significant.
```

5. Now fit a model predicting mortality rate using levels of nitric oxides, sulfur dioxide, and hydrocarbons as inputs. Use appropriate transformations when
helpful. Plot the fitted regression model and interpret the coefficients.

```{r}
pol_t6 <- lm(mort~nox+so2+hc, data = pollution)
plot(pol_t6)
# By observing the dataset, I found some extreme large value in "nox" and "hc", therefore I decided to take log
# on those variables.
pol_t7 <- lm(mort~log(nox)+so2+log(hc), data = pollution)
summary(pol_t7)
plot(pol_t7)
ggplot(pol_t7)+aes(y=pollution$mort, x=log(pollution$nox)+pollution$so2+log(pollution$hc))+geom_point()+geom_smooth(method = "lm")
# log(nox) and so2 have positive correlation with mortality while log(hc) ahs negative correlation.
# Area that has higher nox and so2 tend to has higher mortality rate.
```

6. Cross-validate: fit the model you chose above to the first half of the data and then predict for the second half. (You used all the data to construct the model in 4, so this is not really cross-validation, but it gives a sense of how the steps of cross-validation can be implemented.)

```{r}
poll_30 <- pollution[1:30,]
poll_60 <- pollution[31:60,]
cv_1 <- lm(mort~log(nox)+so2+log(hc), data = poll_30)
pred <- predict(object = cv_1, poll_60, interval="prediction")
pred[,1]-poll_60$mort
```

### Study of teenage gambling in Britain

```{r,message =FALSE}
data(teengamb)
?teengamb
```

1. Fit a linear regression model with gamble as the response and the other variables as predictors and interpret the coefficients. Make sure you rename and transform the variables to improve the interpretability of your regression model.

```{r}
gamb_1 <- lm(gamble~sex+status+income+verbal, data = teengamb)
summary(gamb_1)
plot(gamb_1, which = 1)
gamb_2 <- lm(log(gamble+0.01)~sex+status+income+verbal, data = teengamb)
summary(gamb_2)
plot(gamb_2, which = 1)
# I take log on the respondent variable "gamble". For a male with zero income, zero verbal score and zero status 
# score, the average expenditure on gambling is 1.2495
# Females tend to spend less on gambling than males. For every dollar more in income, the expenditure will be
# increase by 29.8%
```

2. Create a 95% confidence interval for each of the estimated coefficients and discuss how you would interpret this uncertainty.

```{r}
confint(gamb_2, level = 0.95)
# "status" "verbal" and "income" are significant while "sex" might not seem to be as significant as the others
```

3. Predict the amount that a male with average status, income and verbal score would gamble along with an appropriate 95% CI.  Repeat the prediction for a male with maximal values of status, income and verbal score.  Which CI is wider and why is this result expected?

```{r}
# Model for an "average guy"
c_status <- mean(teengamb$status)
c_income <- mean(teengamb$income)
c_verbal <- mean(teengamb$verbal)
agdata <- data.frame(status=c_status,income=c_income,verbal=c_verbal,sex=0)
ag <- predict(gamb_2, newdata = (agdata),level=0.95, interval="confidence")
summary(ag)
# The average guy tends to spend 1.748 on gambling per week.

# Model for a "rich guy"
rgdata <- data.frame(status=max(teengamb$status),income=max(teengamb$income),verbal=max(teengamb$verbal),sex=0)
rg <- predict(gamb_2, newdata = (rgdata),level=0.95, interval="confidence")
summary(rg)
# A guy with maximal status, income and verbal score tends to spend 4.77 dollars on gambling.
```

### School expenditure and test scores from USA in 1994-95

```{r}
data(sat)
?sat
```

1. Fit a model with total sat score as the outcome and expend, ratio and salary as predictors.  Make necessary transformation in order to improve the interpretability of the model.  Interpret each of the coefficient.

```{r}
regsat <- lm(total~expend+ratio+salary, data = sat)
plot(regsat)
# I assume there are interactions between expend and salary
c_ratio <- sat$ratio - mean(sat$ratio)
regsat_2 <- lm(total~expend*salary+c_ratio, data = sat)
plot(regsat_2, which = 1)
summary(regsat_2)
# Intercept: a student from a zero income family, goes to average ratio school and doesn't spend money at school
# likely to have SAT score of 1411. With more expenditure at school will decrease the student's SAT score. If the 
# student's family make more maney, his or her SAT score will also be decrease. However, if the student goes to a 
# school that has higher student/teacher ratio, the student tend to have higher SAT score.
```

2. Construct 98% CI for each coefficient and discuss what you see.

```{r}
confint(regsat_2, level = 0.98)
# All of the variables are not statistically significant
```

3. Now add takers to the model.  Compare the fitted model to the previous model and discuss which of the model seem to explain the outcome better?

```{r}
regsat_3 <- lm(total~expend*salary+c_ratio+takers, data = sat)
plot(regsat_3)
summary(regsat_3)
# I personally prefer this model since it shows "takers" has significant incluence on the outcome, although
# the residual plot is still bad.
```

# Conceptual exercises.

### Special-purpose transformations:

For a study of congressional elections, you would like a measure of the relative amount of money raised by each of the two major-party candidates in each district. Suppose that you know the amount of money raised by each candidate; label these dollar values $D_i$ and $R_i$. You would like to combine these into a single variable that can be included as an input variable into a model predicting vote share for the Democrats.

Discuss the advantages and disadvantages of the following measures:

* The simple difference, $D_i-R_i$

  \color{blue}The difference tells the the difference in amount of money raised by two indivisual candidates. By using this formula, we could easily tell who raise more money and how much in difference.\color{black}
  
* The ratio, $D_i/R_i$
 
  \color{blue}The ratio tells the proportion of amount of money raised by two indivisual candidates. By using this formula, we could easily find out the comparison of "efficiency". In other words, we could know that for every one dollar candidate D raised, how much candidate R could raise.\color{black}
  
* The difference on the logarithmic scale, $log D_i-log R_i$ 

  \color{blue} We could transfor $log D_i-log R_i$ to $ log(D_i/R_i)$. The formula tells us the percentage change in one candidate's fund raise will influence how much on the other candidate's fund raise.\color{black}
  
* The relative proportion, $D_i/(D_i+R_i)$.
 
  \color{blue} The formula tells us the weight of amount money of D raised in the total money raised by both person. By using this method, we could track the fund raising dynamically.\color{black}
  

### Transformation 


 \color{red} See attched photos\color{black}
 
 
For observed pair of $\mathrm{x}$ and $\mathrm{y}$, we fit a simple regression model 
$$\mathrm{y}=\alpha + \beta \mathrm{x} + \mathrm{\epsilon}$$ 
which results in estimates $\hat{\alpha}=1$, $\hat{\beta}=0.9$, $SE(\hat{\beta})=0.03$, $\hat{\sigma}=2$ and $r=0.3$.

1. Suppose that the explanatory variable values in a regression are transformed according to the $\mathrm{x}^{\star}=\mathrm{x}-10$ and that $\mathrm{y}$ is regressed on $\mathrm{x}^{\star}$.  Without redoing the regression calculation in detail, find $\hat{\alpha}^{\star}$, $\hat{\beta}^{\star}$, $\hat{\sigma}^{\star}$, and $r^{\star}$.  What happens to these quantities when $\mathrm{x}^{\star}=10\mathrm{x}$ ? When $\mathrm{x}^{\star}=10(\mathrm{x}-1)$?


2. Now suppose that the response variable scores are transformed according to the formula
$\mathrm{y}^{\star\star}= \mathrm{y}+10$ and that $\mathrm{y}^{\star\star}$ is regressed on $\mathrm{x}$.  Without redoing the regression calculation in detail, find $\hat{\alpha}^{\star\star}$, $\hat{\beta}^{\star\star}$, $\hat{\sigma}^{\star\star}$, and $r^{\star\star}$.  What happens to these quantities when $\mathrm{y}^{\star\star}=5\mathrm{y}$ ? When $\mathrm{y}^{\star\star}=5(\mathrm{y}+2)$?


3. In general, how are the results of a simple regression analysis affected by linear transformations of $\mathrm{y}$ and $\mathrm{x}$?



4. Suppose that the explanatory variable values in a regression are transformed according to the $\mathrm{x}^{\star}=10(\mathrm{x}-1)$ and that $\mathrm{y}$ is regressed on $\mathrm{x}^{\star}$.  Without redoing the regression calculation in detail, find $SE(\hat{\beta}^{\star})$ and $t^{\star}_0= \hat{\beta}^{\star}/SE(\hat{\beta}^{\star})$.


5. Now suppose that the response variable scores are transformed according to the formula
$\mathrm{y}^{\star\star}=5(\mathrm{y}+2)$ and that $\mathrm{y}^{\star\star}$ is regressed on $\mathrm{x}$.  Without redoing the regression calculation in detail, find $SE(\hat{\beta}^{\star\star})$ and $t^{\star\star}_0= \hat{\beta}^{\star\star}/SE(\hat{\beta}^{\star\star})$.


6. In general, how are the hypothesis tests and confidence intervals for $\beta$ affected by linear transformations of $\mathrm{y}$ and $\mathrm{x}$?



		
# Feedback comments etc.

If you have any comments about the homework, or the class, please write your feedback here.  We love to hear your opinions.

